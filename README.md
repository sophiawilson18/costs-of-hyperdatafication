# How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI

This repository contains reproducible scripts for retrieving metadata for all public Hugging Face datasets. 

---

## Environment Setup
Create the environment using the provided file:
```bash
conda env create -f environment.yml
```

Activate the environment:
```bash
conda activate datalifecycle
```

Log in to Hugging Face:
```bash
huggingface-cli login
```

Verify the login and token:
```bash
hf auth whoami
cat ~/.cache/huggingface/token
```

## 1. Retrieve Metadata 
### 1.1 Download all dataset IDs
Fetch the full list of public dataset IDs from Hugging Face:
```bash
python fetch_all_ids.py
```

### 1.2 Fetch repository-level metadata
Repository-level attributes (timestamps, downloads, and Hub-side storage) are collected via fetch_metadata_replevel.py.
For scalability, the list of dataset IDs can be split into multiple parts.

```bash
python fetch_metadata_replevel.py \
  --ua "Comment" \
  --ids-file all_dataset_ids_A.txt --parts-dir parts --part-prefix A \
  --out core_A.parquet --threads 4 --sleep 0.2 --batch-size 1000
```

### 1.2 Fetch data-level metadata
Dataset size is fetched separately using:
```bash
python fetch_metadata_dataset_size.py \
  --ua "Comment" \
  --ids-file all_dataset_ids_A.txt --parts-dir parts --part-prefix A \
  --out core_A.parquet --threads 4 --sleep 0.2 --batch-size 1000
```

### 1.3 Fetch contextual metadata
Contextual attributes (region, modality, task, and language) retrieved using separate scripts:
```bash
python fetch_metadata_regions.py
python fetch_metadata_modalities.py
python fetch_metadata_tasks.py
python fetch_metadata_languages.py
```

## Data availability

The resulting Parquet files are not included in this repository.
They can be regenerated by running the scripts above.

## License 
Code is released under the CC BY 4.0 licence.



